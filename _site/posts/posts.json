[
  {
    "path": "posts/2021-01-24-statistical-dignificance-by-betting-reframing-the-p-value/",
    "title": "Statistical Significance by Betting: Re-framing the p-value?",
    "description": {},
    "author": [],
    "date": "2021-01-24",
    "categories": [],
    "contents": "\nThe p-value, originally developed over 100 years ago by Karl Pearson, has long been a staple of statistical hypothesis testing, but it has become an increasingly controversial topic in recent decades. In 2019, an article titled “Scientists rise up against statistical significance”, which was co-signed by over 800 statistical researchers, was published in Nature, bringing the topic to the forefront of conversations across the sciences.\nThis controversy has reasonably become a frequent topic of discussion in undergraduate statistics courses, and I have become intrigued by potential solutions. In this post I write about one of the most interesting ones that combines elements of economics and game theory: the proposition that we begin to think of p-values as bets.\nThe Pros and Cons of Statistical Significance\nThe p-value itself is not an erroneous concept. The issue is that it is so frequently misused and poorly understood that a far too large portion of published scientific conclusions are misguided. This problem arises from the p-value’s close relation to and another concept that is erroneous in nature: statistical significance. While a p-value is simply the probability that the observed results (or more extreme ones) would have occurred under the null hypothesis, statistical significance is the idea that if a p-value lies under a pre-defined threshold (often 0.05 in the biomedical sciences), then the result is “significant”, otherwise is it not.\nOn the surface, statistical significance seems like a reasonable idea, especially considering that it requires that the threshold for significance is pre-defined to prevent researchers from moving it around to force their hypotheses to be “significant”. It also allows researchers to present their results in a more convincing manner (publishing results that says this drug most likely works doesn’t cut it for many), while not making conclusions completely certain because significant ≠ definitely true.\nUnfortunately, no matter how many times statisticians try to hammer the previous inequality into the minds of their colleagues and readers, significance is frequently interpreted as a definite answer. This manner of thinking stems from our (evolutionarily selected for) human desire for certainty, which causes us to make things more black-and-white than they truly are. Thus, if p = 0.049, we reject the null hypothesis and claim our results to be “significant”, but we can’t draw a meaningful conclusion if p = 0.051 even though the difference between these two p-values is really next to meaningless. On top of that, significance is rarely presented in levels, meaning a test that yields a p-value of 0.001 is frequently interpreted to be just as convincing as a test that yields a p-value of 0.04.\nThe current use of the p-value in research also leads to problems when multiple comparisons are involved. While methods such as the Bonferroni correction are frequently used to address these issues at the experimental level, they are more difficult to avoid on larger scales. For example, if 20 different researchers groups study a hypothesis that is not actually true, we can expect one to conclude that the hypothesis is actually significant based on chance alone. When research publication processes favor studies with “significant” results, they are asking for false hypotheses to be presented and accepted by the scientific community. This demonstrates why many research communities are facing replication crises.\nDespite these concerns, some researchers have defended statistical significance by comparing it to concepts like “beyond reasonable doubt” in courts. They point out that we can never know anything with absolute certainty no matter how much evidence there is, so we must live with threshold of significance that helps us get most things right, even if it means risking some incorrect conclusions simply due to chance.\nUltimately, both of these arguments have merit. A major issue in scientific research is incorrect interpretation of p-values and statistical significance, so better education and presentation is certainly needed. But how much will that really change? If we did decide to forget the concept of significance, is there a better alternative?\nMaybe all we need to do is re-frame it.\nDecisions as Bets\nAs bestselling author and elite poker player Annie Duke points out, decisions are simply bets on the future. We can never know for sure which choice will result in the better outcome, but we make decisions based on the evidence we have and are willing to put more at stake as our perceived probability of a positive outcome increases. As a simple example, I am willing to drive for an hour to visit my friend because based on my assessments (conscious or subconscious), I am so much more likely to have an enjoyable time with him than I am to get in a car accident on the way that the decision to drive is worth it. Plus, staying home by myself won’t be very enjoyable. In contrast, if the weather forecast suggested heavy snow, I might decide that the the enjoyment I would get out of seeing my friend is no longer worth the risk of getting in a car accident. Every decision we make, including which cereal we want to eat for breakfast, follows this same process (although they are subject to frequent errors in our risk/benefit calculations).\nWhen risk calculations are well thought out, making decisions as bets on the future will maximize our chances of favorable outcomes. We will of course get unlucky sometimes, but in the long run we are increasing our chances of success.\nNow, this begs the question: why aren’t scientists treating their research decisions as bets? By using a uniform p-value threshold for significance, we are severely oversimplifying the decision-making process by:\nMaking it difficult for audiences to comprehend the strength of the evidence beyond a binary yes/no.\nEssentially treating every research project as being equally important.\nIn later sections, I refer to these points as oversimplifications 1 and 2.\nThe current scientific standard is that we are willing to accept a 5% chance of Type I error (incorrectly rejecting the null hypothesis) for every biomedical study. In terms of the friend-visiting example, that is like me deciding that I will only make the drive if there is a 50% chance or less of snow, regardless of whether the friend is a distant acquaintance or my lifelong best friend on his deathbed. Clearly, my willingness to risk driving should depend on the context of who the friend is. Even more, if I had to defend my decision to someone else (say, for example, my worried mother), she would certainly react differently if the chance of snow was 49% compared to 0.01%. Is scientific research any different?\nThe concept of statistical significance takes research findings out of their context. Re-framing p-values as bets may lead to better overall research outcomes by bringing the context of the work into the decision.\nGlenn Shafer’s Betting Scores\nAs I became more interested in this idea of re-framing the p-value as a bet, I stumbled upon the Rutgers-Royal research group. Their project titled “Game-Theoretic Probability and Finance” has been ongoing for over two decades, and they have published a number of interesting papers online, many of which can be found here. Glenn Shafer, a Mathematical Statistician from The University of Rutgers, is a leader of this group. In this section I summarize some of his basic points from this article that specifically tackle issues with the current use of the p-value. Note that the approaches outlined in this section relate primarily to oversimplification 1 from the previous section: the problem of binary (yes/no) significance. It also helps to address problems with multiple comparisons and can be extended to incorporate many more factors\nThe fundamental principle of Dr. Shafer’s alternative to statistical significance is what he calls a betting score. This is defined as:\n\\[\\frac{S(y)}{\\mathbf{E}_p(S)}\\]\nwhere:\n\\(S(y)\\) is the payoff received for y occurring\n\\(\\mathbf{E}_P(S)\\) is the expected value1 of the bet under the null hypothesis (which Shafer calls \\(P\\)), or the “money” put at risk.\nThus, we can call \\(S\\) the bet. \\(S\\) is a (non-negative) function.\nSince constant multipliers of the bet do not change the ratio (i.e. the betting score is the same whether $1 or $5 is bet, just as betting $100 vs. $150 on your favorite football does not change the bookmaker’s odds), Shafer assumes that \\(\\mathbf{E}_p(S)\\) = 1 for simplicity. In this case, the betting score is just \\(S(y)\\).\nIn addition to making things simpler, setting \\(\\mathbf{E}_p(S)\\) = 1 also conveniently means that \\(SP\\) is a probability distribution defined by the non-negative bet (\\(S(y)\\)) under the null hypothesis (\\(P\\), which is itself a probability distribution). This distribution defines the alternative to P according to the bet, which can be called Q. By this definition, \\(S(y)\\) = \\(Q(y)/P(y)\\), so Shafer shows us that the betting score is a likelihood ratio of the alternative hypothesis to the null hypothesis.\nUnder this simple framework, researchers may present their results as betting scores against the null hypothesis. In other words, they show how much money a bet for Q with an expected value of $1 under the null hypothesis has actually returned based on the scientific evidence. Clearly, higher payoffs mean that the research has presented stronger evidence that the null hypothesis is incorrect.\nA key added bonus of this betting score method is that successive tests can be assessed simply by multiplying betting scores. That is, the money won from the earlier tests (and no more) is used to “buy” a subsequent bet. Here, the accumulated return can be used to assess the overall evidence, which helps to address multiple comparison issues and the closely related replication crisis.\nSo does Dr. Shafer’s testing by betting proposal eliminate the idea of significance altogether? Not really. No matter how a statistical hypothesis test is framed, a sound one requires a way to evaluate it before it is run given the hypotheses. Under the traditional p-value framework, this is where the idea of statistical power comes in. Recall that power tells us the probability that the test will reject the null hypothesis if it is truly incorrect. Under Shafer’s framework, hypothesis tests are set up beforehand with an implied target, \\(S^{\\ast} = \\exp(\\mathbf{E}_Q(\\ln{S}))\\) (see Dr. Shafer’s paper in the references for details about where this comes from). This value is betting score that is defined by the bet and null hypothesis. If Q is correct, this implied target is the expected betting score. Thus, a good test must have a high implied target (For reference, Shafer points out that a standard test always has an implied target of 20 if \\(\\alpha\\) = 0.05) and a reasonable Q. If these conditions are met, the betting score is a valuable, publishable result regardless of how strong it is.\nConsidering Research Impact\nWhile Glenn Shafer’s theory of betting scores and implied targets helps to address oversimplification 1, the current framework does not discuss the second oversimplification that the current use of statistical significance suffers from: the issue that every project is essentially treated the same.\nMany statisticians will argue that this is in fact a strength of statistical significance, and there are certainly strong arguments supporting this notion. To highlight how it can sometimes be problematic, though, I present the following examples: suppose that an efficacy trial for a pandemic-saving vaccine is run, but the pressure of time and difficulties enrolling participants in the study decreases its statistical power. The study concludes, and the statisticians analyze the data as the public waits eagerly. Finally, the results are published and show some evidence that the vaccine is effective, but p = 0.08. The research team offers to continue testing, but it would be very costly in addition to having methodological problems. Even though the increased efficacy compared to the placebo was not “statistically significant”, how should the results be interpreted?\nIn contrast, let’s now suppose that a vaccine for a rare, non-fatal disease is being tested in a pandemic-free world. A clinical trial with this somewhat risky vaccine is run and yields the exact same results as the study from the previous example. Again, p = 0.08.\nDespite yielding the same statistical results, the context of these two example studies clearly calls for their results to be interpreted differently by society (the stakeholders in the outcome). While the differences between most research projects are not as stark as the differences in these example scenarios, no two studies have the exact same potential risk/benefit to society. Even though researchers, statisticians, and policy makers certainly consider societal impacts in their current interpretations of research, 1) the all-or-nothing framing of statistical significance abets misjudgments by readers and 2) the way in which the potential impact is incorporated with the results is not standardized, nor is it a part of the actual research process. Instead, the interpretation beyond the p-value is often left to non-technical policymakers. Would it not make more sense to include these valuations as part of the research results? If they were to be included, testing by betting would be a perfect framework by which to incorporate them. Turning back to Shafer’s proposals, the pre-determined potential societal impact could be incorporated into the bet and thus also the implied target.\nThe Freerolling Problem in Science\nI had difficult time finding theoretical work on a “betting” model for scientific research that would incorporate the potential societal impact. After some digging, though, I came across a very brief discussion of this issue here by a colleague of Shafer’s at Rutgers, Harry Crane. In this mini-talk he uses the term “freerolling”, a word used in gambling to describe betting with a chance to win money but no chance of losing, to help illustrate why research results are so often misrepresented and how framing results as bets could be useful.\nTaking a step back to the earlier example of my decision to visit my friend, the freerolling issue would occur if a third individual who doesn’t care whether or not I get into a car accident makes the decision for me. In fact, this individual would be rewarded if I made the trip but not punished at all if I didn’t. Researchers, specifically statisticians, often very little at stake in regards to the implication of their results. If they submit “significant” results to a journal, they have a greater chance of being published, improving their reputation, and feeling that the work was meaningful. They clearly have a chance to benefit from the claim of “significance”, but if later evidence disproves their results, they won’t face any major consequences and can save their reputation by calling it an unlucky case of type II error. Meanwhile, the patients/population who had a direct stake in the study results may face more grave consequences.\nIn this sense, publishing “significant” results in scientific research is clearly a freeroll bet. Researchers have a chance at being rewarded for publishing misleading results without any chance of being punished. This incentives practices such as “p-hacking” and data over-analysis. So how do we combat this? One approach would be to force researchers to have a stake in their results by betting on them. This idea differs from Shafer’s betting score theory, which is based on conceptual bets as a way of capturing confidence in results, but could still be incorporated as an extension.\nConclusion\nThe message of this post is not that there is something wrong with p-values. The problem is that the binary, all-or-nothing use of statistical significance that doesn’t take into account the context of the study question is often incorrectly used to interpret them. While Glenn Shafer’s proposed betting score approach combats the issue of binary classification, it doesn’t account for the potential study impact and thus would treat the results from the two examples above the exact same. It seems that these significance by betting theories could possibly be extended to also differentiate between studies with different risk/benefit distributions.\nAs an aspiring statistician, I am fascinated not only by the way in which data is analyzed, but also by the way in which it is interpreted and meaningfully presented. My limited experience in statistical research has shown me the limitations that the concept of statistical significance presents when it comes to this task of presenting data. I wrote this post to document some interesting ideas that I discovered on this topic, but I am not necessarily in favor of all of them. It will be exciting to to see how this conversation develops in the future!\nReferences\nCrane, H. (2020). Response to Glenn Shafer’s Testing by betting: A strategy for statistical and scientific communication [presented at the Royal Statistical Society Conference]. https://www.youtube.com/watch?v=qKWF737Av2o\nShafer, G. (2019). The language of betting as a strategy for statistical and scientific communication. arXiv preprint arXiv:1903.06991.\nShafer, G. (2020). Testing by betting: A strategy for statistical and scientific communication [presented at the Royal Statistical Society Conference]. https://www.youtube.com/watch?v=qKWF737Av2o\nShafer, G. (2021). Let’s replace (p-value + power) with (betting outcome + betting target) [presented at NC State University]. https://www.youtube.com/watch?v=rnS08IRubGM\nNotes\n1Expected Value is the weighted average, or mean, of a random variable. In the context of betting, take a situation where the future has two discrete outcomes. Let’s say that there is a 10% chance that I win $20 and a 90% chance that I lose $1. The expected value of this bet is 0.1*20 + 0.9*-1 = $1.1.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-02-07T16:36:38-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-28-what-if-some-vaccines-are-more-effective-than-others/",
    "title": "Modeling COVID-19 and Vaccine Rollout Strategies",
    "description": {},
    "author": [],
    "date": "2020-12-28",
    "categories": [],
    "contents": "\nOver the past three weeks, the United States government has approved two different COVID-19 vaccines for emergency use. These two vaccines, one developed by Pfizer and the other by Moderna, are actually quite similar in that they are both two-dose mRNA vaccines with mild side-effects, and they have almost identical efficacy rates (95% and 94.1% respectively according to the FDA). The only notable difference is that the Moderna vaccine can be stored in a standard freezer while the Pfizer vaccine must be stored at -70 degrees Celsius, making it slightly less accessible for small healthcare centers.\nConsequently, U.S. vaccine rollout plans have so far treated the two vaccines as essentially being the same, that is to say neither of the two vaccines is considered better than the other. This somewhat unexpected fact has simplified the already controversial rollout, but the impending introductions of new vaccines from companies like Johnson & Johnson and AstraZaneca threaten to introduce a new complexity. If a new vaccine with only 70% efficacy becomes available, how should it be incorporated into the rollout? Is it better to give at-risk patients a worse vaccine if it means they can get vaccinated a month earlier? Will people even want it?\nThe answers to these questions are certainly complex, and they will likely draw from principles in biology, mathematics, statistics, and economics among other fields. I cannot pretend to be an expert here - I am hopeful that the true experts have been thinking deeply about this issue - but I think the problem is quite fascinating. This post contains my thoughts and some basic modeling!\nWhat is Vaccine Efficacy?\nBefore moving any further, let’s make sure we understand what vaccine efficacy is. Essentially, it is a measure of how well a vaccine prevents cases of the disease in a group of vaccinated subjects compared to a control group of un-vaccinated (received placebo) subjects. An estimate of the efficacy is generated from clinical trial data, which every vaccine must collect and submit to the FDA for review before being made available to the public.\nThe actual calculation of efficacy relies on a term called risk. In epidemiology, risk is the probability that an event (such as infection) will occur in a given time period, and it is measured in a clinical trial as the proportion of total subjects in a group who experienced the event in the time period that they were followed. For example, if 8 out of 1,000 patients who receive an actual vaccine in a clinical trial become infected with COVID-19, then the estimate of risk of infection in the vaccination group is simply 0.008, or 0.8%. The ratio of risk in the treatment (actual vaccine) group compared to the risk in the control (placebo) group is called the risk ratio. Risk ratios below 1 suggest that the treatment reduces the risk of disease compared to the control, and smaller ratios suggest a stronger reduction. Efficacy is calculated as 1 - Risk Ratio, so an efficacy closer to 1 (or 100%) means that the vaccine reduces the risk of infection by a greater proportion. You can read more about vaccine efficacy on the CDC website here.\nRemember that ratios are multiplicative, not additive, so we can intepret 95% vaccine efficacy (the equivalent to 100%-95% = 5% risk ratio) as meaning that the risk for COVID in the vaccine group is 0.05 (5%) times the risk in the control group.\nA Simple Model\nTo assess the potential effects of different vaccine rollouts, we will create a model of the COVID-19 pandemic in the United States. Our first step will be to develop a simple framework using principles from epidemiology before adding more complex features. For this simple model, we divide the population of the United States into four groups: susceptible (S), infected (I), recovered and immune (R), and deceased (D). At a specific point in time, every individual should fall into one and only one of these categories.\nWith these categories established, our next job is to determine a way to describe the flow of individuals from group to group. See Diagram 1 below:\n\n\n\n{\"x\":{\"diagram\":\"digraph {\\n  graph [layout = dot, rankdir = LR]\\n  \\n  node [shape = rectangle, fixedsize = true, width = 4.0, height = 2.0, fontname = Helvetica, fontsize  = 20]   \\n  rec1 [label = <<b>Susceptible Individuals (S):<\\/b>>]\\n  rec2 [label = <<b>Infected Individuals (I):<\\/b>>]\\n  rec3 [label = <<b>Recovered and Immune <br/> Individuals (R): <br/><\\/b>>]\\n  rec4 [label = <<b>Deceased Individuals (D):<\\/b>>]\\n  \\n  # edge definitions with the node IDs\\n  rec1 -> rec2 rec2 -> rec3 rec2 -> rec4 rec3 -> rec1 \\n\\n  }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\n\nWe are interested in how the flow of individuals occurs over time, so we will measure it as the rate at which people exit/enter a group per day. Below we explore each arrow on the diagram above:\nS → I: The rate at which susceptible individuals become infected. This value intuitively depends on the rate of contact between susceptible and infected individuals as well as the probability that the disease would be transmitted given a contact occurs. Here we introduce our first parameter, \\(\\beta\\), which captures the rate of transmission. We can calculate \\(\\beta\\) using \\(R_0\\), the more intuitive “basic reproductive rate” of a disease, \\(\\gamma\\), the recovery rate (which is calculated as as 1/length of infectious period), and \\(N\\), the population size, because:\n\\[\\begin{align*}\n\\beta = \\frac{R_0 \\gamma}{N}\n\\end{align*}\\]\n       Thus, S → I is described by \\(\\beta S I\\) where \\(S\\) and \\(I\\) are the number\n       of individuals in groups S and I.\nI → D: The rate at which infected individuals die. This depends on our second parameter, \\(\\gamma\\), and a third parameter, \\(\\mu\\), the death rate. As the inverse of the infectious period, \\(\\gamma\\) captures the average number of infected individuals that stop being infectious each day (i.e. if the infectious period is 5 days, then 1/5 of infectious individuals stop being infectious each day). As the death rate, \\(\\mu\\) tells us what fraction of individuals leaving \\(I\\) are headed to \\(D\\) (what percent die), while the rest, or 1-\\(\\mu\\), recover and head to \\(R\\). Thus, I → D is described by \\(\\gamma I\\mu\\).\nI → R: The rate at which infected individuals recover. Using logic from I → D, above we have I → D is described by \\(\\gamma I(1-\\mu)\\). Note that the total rate at which individuals exit \\(I\\) is simply \\(\\gamma I\\) because every infected individual either dies or recovers.\nR → S: The rate at which recovered individuals lose immunity and become susceptible again. This value depends on a fourth paramter, \\(\\omega\\), the inverse of the period of immunity. It is the same idea as \\(\\gamma\\).\nWith these rates determined, we can now create a system of ordinary differential equations to describe the flow of people between groups. For each group, the change in size with respect to time (the derivative) is simply the sum of the rates with which people enter it (arrows entering the box) minus the rates with which people leave (arrows leaving the box). Thus, we have:\n\\[\\begin{align*}\n\\frac{dS}{dt} &= -\\beta S I + \\omega R \\\\ \n\\frac{dI}{dt} &= \\beta S I - \\gamma I\\\\\n\\frac{dR}{dt} &= \\gamma I(1-\\mu) - \\omega R\\\\\n\\frac{dD}{dt} &= \\gamma I(\\mu)\n\\end{align*}\\]\nwhere the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), and \\(\\mu\\) are the transmission rate, recovery rate, loss of immunity rate, and death rate, respectively. If we set values for each of these parameters based on our knowledge of the COVID-19 pandemic and set initial sizes for each group, we can track the course of the disease by solving this system of ordinary differential equations for each day to model how the disease might progress.\nAdding Long-Term Immunity: Vaccines\nNow that we have established the basic theoretical framework of the model, we can start to add complexities such as vaccines. Vaccinated individuals will come out of the susceptible group and enter a new group with different properties from the four we established before. See Diagram 2 below:\n\n\n\n{\"x\":{\"diagram\":\"digraph {\\n  graph [layout = dot, rankdir = LR]\\n  \\n  node [shape = rectangle, fixedsize = true, width = 4.0, height = 2.0, fontname = Helvetica, fontsize  = 20]   \\n  rec1 [label = <<b>Susceptible Individuals (S):<\\/b>>]\\n  rec2 [label = <<b>Vaccinated Individuals (V):<\\/b>>]\\n  rec3 [label = <<b>Infected Individuals (I):<\\/b>>]\\n  rec4 [label = <<b>Recovered and Immune <br/> Individuals (R): <br/><\\/b>>]\\n  rec5 [label = <<b>Deceased Individuals (D):<\\/b>>]\\n  \\n  # edge definitions with the node IDs\\n  rec1 -> rec2 rec1 -> rec3 rec3 -> rec4 rec3 -> rec5 rec4 -> rec1 \\n\\n  }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\n\nThis model assumes that every vaccinated individual is protected completely from infection, but we know that this is not the case for COVID-19 vaccines in the real world. Accounting for this fact and supposing that individuals can receive one of two different vaccines, we have a new model detailed in Diagram 3.\n\n\n\n{\"x\":{\"diagram\":\"digraph {\\n  graph [layout = dot, rankdir = LR]\\n  \\n  node [shape = rectangle, fixedsize = true, width = 4.0, height = 2.0, fontname = Helvetica, fontsize  = 20]   \\n  rec1 [label = <<b>Susceptible Individuals (S):<\\/b>>]\\n  rec2 [label = <<b>Vaccine 1 (V1):<\\/b>>]\\n  rec3 [label = <<b>Vaccine 2 (V2):<\\/b>>]\\n  rec4 [label = <<b>Infected Individuals (I):<\\/b>>]\\n  rec5 [label = <<b>Recovered and Immune <br/> Individuals (R): <br/><\\/b>>]\\n  rec6 [label = <<b>Deceased Individuals (D):<\\/b>>]\\n  \\n  # edge definitions with the node IDs\\n rec1 -> rec2 rec1 -> rec3 rec2 -> rec4 rec3 -> rec4 rec1 -> rec4  rec4 -> rec5 rec4 -> rec6 rec5 -> rec1\\n\\n  }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\n\nAssuming that the number of vaccines given per day, \\(\\nu_1\\) and \\(\\nu_2\\), and the rates of success, \\(\\varphi_1\\) and \\(\\varphi_2\\), for each vaccine are known, we now have the following system of ordinary differential equations:\n\\[\\begin{align*}\n\\frac{dS}{dt} &= -\\beta S I + \\omega R -  \\nu_1  -  \\nu_2 \\\\\n\\frac{dV_{1}}{dt} &= \\nu_1 - (1-\\varphi_1) \\beta V_1 I\\\\\n\\frac{dV_{2}}{dt} &= \\nu_2 - (1-\\varphi_2) \\beta V_2 I \\\\\n\\frac{dI}{dt} &= \\beta S I - \\gamma I + (1-\\varphi_1)\\beta V_{1} I + (1-\\varphi_2)\\beta V_{2} I \\\\ &= \\beta I[S + (1-\\varphi_1)V_{1} + (1-\\varphi_2)V_{2}] - \\gamma I\\\\\n\\frac{dR}{dt} &= \\gamma I(1-\\mu) - \\omega R\\\\\n\\frac{dD}{dt} &= \\gamma I(\\mu)\n\\end{align*}\\]\nThere are obviously a number of assumptions that this model is making for the sake of simplicity. The big ones are:\nEvery individual has the same \\(\\beta\\), that is the same probability of coming into contact with an infected individual and then contracting it.\nActually, all the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), \\(\\mu\\), \\(\\nu_1\\), \\(\\nu_2\\), \\(\\varphi_1\\), and \\(\\varphi_2\\) are constant throughout the population and over time.\nThe population is closed other than COVID deaths. No births or deaths from other causes are accounted for.\nWe should be okay with most of these assumptions for the sake of our simple model. Accounting for the ways in which behavioral changes, weather patterns, and government policies change the parameters would help the model better reflect the real life course of the virus, but we will leave that to the experts. With that said, there is one key factor that is currently being ignored - age.\nIt is well known that virus impacts different age groups in very different ways. In terms of our parameters, the death rate, \\(\\mu\\), varies greatly across age groups. On top of that, the vaccine rollout plans (impacting \\(\\nu_1\\) and \\(\\nu_2\\)) will also certainly account for differences in age. In the following section, I discuss how we can incorporate age classes into our model.\nAdding Age Classes\nLet’s now suppose that our population is made up of distinct age classes where \\(N_i\\) is the number of individuals in age group \\(i\\). For our purposes, we will use the age classes (with their corresponding N and death rates) detailed below:\n\n.cl-f101b030{font-family:'Arial';font-size:11px;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;}.cl-f101b09e{font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;}.cl-f101b0bc{font-family:'Arial';font-size:7px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;position: relative;bottom:3px;}.cl-f101b0d0{font-family:'Arial';font-size:6px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3px;}.cl-f101b0da{font-family:'Arial';font-size:10px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f101f842{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-f101f888{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-f101f89c{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1.00;background-color:transparent;}.cl-f10269d0{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a02{width:76px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a16{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a2a{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a3e{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a52{width:76px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a5c{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a70{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a84{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026a8e{width:76px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026aa2{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026aac{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026ac0{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026aca{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026ade{width:76px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026ae8{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026afc{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026b06{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026b1a{width:76px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f1026b24{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}iAgePopulation (in Millions)Death Rate1 0-4 19.58 0.00026 2 5-9 20.19 0.00010 3 10-14 20.80 0.00010 4 15-19 21.06 0.00022 5 20-25 21.63 0.00040 6 26-30 23.50 0.00040 7 31-35 22.43 0.00135 8 36-40 21.73 0.00135 9 41-45 19.92 0.00346 10 46-50 20.40 0.00346 11 51-55 20.48 0.01267 12 56-60 21.87 0.01267 13 61-65 20.57 0.01267 14 66-70 17.46 0.04820 15 71-75 14.03 0.04820 16 76-80 9.65 0.11636 17 81-85 6.32 0.11636 18 85+ 6.61 0.22083  Death Rate is calculated as overall deaths rate of population divided by overall incidence rate in population according to the CDC's ACIP report on 12/20/20. Population totals are for 2019 from Statistica.com.\n\nThe seemingly simple, however incorrect, approach would be to run our previously specified vaccine model on each age group individually and calculate the overall size of each group as the sum of its size in each age group. For example, \\(S = \\sum_{i=1}^{18} S_i\\). The problem with this approach is that it assumes that individuals only interact with, and therefore can only be infected by, people in the same age class. This is clearly not reflective of the real world.\nTo solve this issue, we need a contact matrix. A contact matrix describes the contacts between age group \\(i\\) and age group \\(j\\). As an example, let’s suppose that there are 900 students and 100 teachers in a school. Students make on average 40 contacts with other students and 10 contacts with teachers per day. Since the ratio of students to teachers is 9:1, students making 10 contacts with teachers on average means that teachers make on 10*9 = 90 contacts with students per day an average. Now also assume that the teachers only make contact with 5 other teachers per day on average. The contact matrix would be:\n\\[\n\\begin{bmatrix}\n40 & 10 \\\\\n90 & 5 \\\\\n\\end{bmatrix}\n\\]\nThese contact matrices have a few cool properties. You can read more about them here.\nBecause of the importance of the student-to-teacher ratio in the example above, contact matrices based on one demographic or social structure are are not generally compatible with other structures. Conveniently, a 2017 study by Kiesha Prem, Alex Cook, and Mark Jit estimated contact matrices for 152 countries, including the United States, using contact surveys and demographic data. We can download their contact matrix for the United States, multiply it by 0.2 to account for the reduced social mixing due to social distancing, and include it as a new parameter, C, in our model. You can view the code for the creation of C and the rest of this analysis at https://github.com/yangjasp/distill_site.\nFor our system of ordinary differential equations we now have:\n\\[\\begin{align*}\n\\frac{dS_i}{dt} &= -\\beta S_i C (I_i/N) + \\omega R_i - \\nu_{1i}  -  \\nu_{2i} \\\\\n\\frac{dV_{1i}}{dt} &=  \\nu_{1i} - (1-\\varphi_1) \\beta V_{1i} C (I_i/N)\\\\\n\\frac{dV_{2i}}{dt} &=  \\nu_{2i} - (1-\\varphi_2) \\beta V_{2i} C (I_i/N)\\\\\n\\frac{dI_i}{dt} &= \\beta S_i I_i - \\gamma I_i + (1-\\varphi_1)\\beta V_{1i} C (I_i/N) + (1-\\varphi_2)\\beta V_{2i} C (I_i/N) \\\\\n\\frac{dR_i}{dt} &= \\gamma I_i(1-\\mu_i) - \\omega R_i\\\\\n\\frac{dD_i}{dt} &= \\gamma I_i(\\mu_i)\n\\end{align*}\\]\nNote that this model assumes that the parameters \\(\\beta\\), \\(\\gamma\\), \\(\\omega\\), \\(\\varphi_1\\), and \\(\\varphi_2\\) are constant across all age classes, while the death rate, \\(\\mu_i\\), and the vaccination rates, \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\), vary across classes.\nCompleting the Final Model\nIntroducing a contact matrix allows our model to more accurately capture the rate of transmission across age classes, but it does not account for the fact that asymptomatic carriers are much more likely to come into contact with susceptible individuals than symptomatic ones. With this fact in mind, we introduce one more parameter, \\(\\alpha_i\\), to represent the fraction of asymptomatic infections per age class. This addition should be an important one, as it will help our model better capture the reported role of young adults as viral spreaders. Similarly to the approach described in a 2012 paper by Tower and Feng, we can now split our infected (Is) classes into symptomatic (Is) and asymptomatic (Ia) class. We will also create a new contact matrix, Cs, which for our purposes will be a our old contact matrix, now called Ca, multiplied by some constant \\(k\\) < 1 which represents the fraction of contacts that a symptomatic individual makes compared to an asymptomatic individual on average. This gives us a revised final system of ordinary differential equations:\n\\[\\begin{align*}\n\\frac{dS_i}{dt} &= -\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\omega R_i - \\nu_{1i}  -  \\nu_{2i} \\\\\n\\frac{dV_{1i}}{dt} &=  \\nu_{1i}  - \\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] \\\\\n\\frac{dV_{2i}}{dt} &=  \\nu_{2i}  - \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\\\\n\\frac{dI^{a}_i}{dt} &= \n\\alpha_i\\Big[\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\\\ &\\phantom{=}\n\\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] +  \\\\ &\\phantom{=}  \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\Big] - \n\\gamma I^{a}_i \\\\\n\\frac{dI^{s}_i}{dt} &= (1-\\alpha_i)\\Big[\\beta S_i [C^{a} (I^{a}_i/N_i) + C^{s} (I^{s}_i/N_i)] + \\\\ &\\phantom{=}\n\\beta (1-\\varphi_{1}) V_{1i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)] +  \\\\ &\\phantom{=}  \\beta (1-\\varphi_{2}) V_{2i} [C^{a} (I_i/N_i) + C^{s}(I^{s}_i/N_i)]\\Big] - \n\\gamma I^{s}_i \\\\\n\\frac{dR_i}{dt} &= (1-\\mu_i)(\\gamma I^{a}_i + \\gamma I^{s}_i) - \\omega R_i\\\\\n\\frac{dD_i}{dt} &= \\mu_i(\\gamma I^{a}_i + \\gamma I^{s}_i)\n\\end{align*}\\]\nWhere \\(C^{s} = k C^{a}\\) in our case (it doesn’t have to in all cases, clearly, but we will use this for simplicity). This is our final model.\nRunning the Model\nWe can now use this model to answer our original question about the rollout of vaccines with different efficacies by adjusting the rates at which they are given in each age class, \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\).\nSo how do we set \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\)? According to a recent Bloomberg article, approximately 200,000 people per day are being vaccinated against COVID-19 on average. This number is pretty low, and some experts have said that this number needs to be much larger if the U.S. wants to reach its goals in 2021. Let’s say that the average number of vaccinations per day increases to 1 million in the new year. Since we defined \\(\\nu_{1i}\\) and \\(\\nu_{2i}\\) as the number of susceptible patients vaccinated per day, we can set a limit on the total vaccinations per day, \\(1,000,000 = \\sum_{i=1}^{18} \\nu_{1i}+\\nu_{2i}\\). Here, we assume that the limiting factor in doses per day is the administration of them, not the availability of the vaccines themselves. Right now, this appears to be a more than reasonable assumption.\nNote: for the models that follow, I will be using the following parameters:\nParameter\nValue\n\\(\\beta\\)\n0.05\n\\(\\gamma\\)\n1/7\n\\(\\omega\\)\n1/91\n\\(\\varphi_1\\)\n0.95\n\\(\\varphi_2\\)\n0.70\n\\(k\\)\n0.20\nScenario 1: Only One Vaccine Available\nLet’s start by supposing that only one vaccine efficacy, 95%, is available. Remembering our discussion about vaccine efficacy, this means that compared to people who are not vaccinated, those who have been vaccinated have 0.05 times the risk of getting COVID. Assuming vaccinated individuals are making the same social contacts as they would if they were not vaccinated, this means that we have \\(\\sum_{i=1}^{18} \\nu_{1i} = 1,000,000\\) (\\(\\nu_{2i} = 0\\) for all \\(i\\)), and \\(\\varphi_{1i} = 0.95\\). That is, people who receive a vaccine receive vaccine 1 and are \\(1-\\varphi_{1i}=0.05\\) times as likely to get COVID as those in the S group.\nBelow is a series of graphs comparing the the overall trajectory of deaths due to COVID-19 in the U.S. with no vaccine, Vaccine 2 (70% Efficacy) alone, and Vaccine 1 (95% Efficacy) alone. In these scenarios, both vaccines are administered starting with the oldest individuals and gradually moving down age classes.\n\n\n\nThe sizeable difference between the three cases is clear and demonstrates the impact that vaccines can have on the COVID-19 pandemic. Both vaccines cut the cumulative death total for 2021 in half and decreased the rate of deaths significantly. If we have an effective vaccine, the rate of COVID deaths starts to slow in late spring, even without a change in our distancing behavior.\n\n\n\nOkay, the results of this scenario were probably pretty obvious. Now for the fun in scenario 2!\nScenario 2: Vaccines of Different Efficacies\nNow let’s suppose that there are two vaccines of different efficacies available. For the approximately 320 million people in the US, we have 150 doses ordered each for Vaccine 1 (95% efficacy) and Vaccine 2 (70% efficacy). In this scenario, we are still limited to 1 million doses per day, but the vaccines can each only be produced at a rate of 500,000 per day (note that if 1 million per day of the 95% could be produced, we would clearly use that on the most vulnerable individuals until we ran out before starting to produce the 70% and using that on whomever remained). We now have the dilemna that inspired this post!\nClearly we want to vaccinate the oldest and most vulnerable individuals first, but is it better to give them the Vaccine 2 if it means they get vaccinated faster? Let’s consider a couple of approaches:\nApproach A : Every individual 65 or older will receive Vaccine 1. Vaccine 2 will only be given to people below 65, starting with the oldest (most vulnerable) and moving down age groups. Once the amount of susceptible individuals that are 65 or older drops below 500,000, the rest of the vaccine 1 doses can be allocated to whoever the oldest individuals left are.\nApproach B: The oldest individuals will be vaccinated as soon as possible with either vaccine. Every day, the oldest 500,000 people will receive Vaccine 1 and the next oldest 500,000 will receive Vaccine 2.\n\n\n\nBased on our model, approach A will save approximately 200,000 more lives overall than approach B. Almost 300,000 lives are predicted to be saved in the age 65+ category, although approach 1 results in more young deaths. This finding suggests that if our goal is to save the most lives, vulnerable individuals need to be vaccinated with an effective vaccine, even if that means it is done at a slower rate.\nBut is it really this simple? Remember that the figure above is based on a Vaccine 1 with 95% efficacy and a Vaccine 2 with 70% efficacy. But what if the difference in efficacies is smaller? The figure below compares the two approaches when Vaccine 2 has an 90% efficacy.\n\n\n\nUnder these conditions, approach B saves more lives overall. This finding is logical when we consider that approach B would clearly be optimal if both Vaccine 1 and Vaccine 2 had the same efficacy. Putting of vaccinations of vulnerable individuals is not worth it if the two vaccines have similar efficacies.\nSo at what Vaccine 2 efficacy does approach B overcome approach A as optimal? At this point of interest, the impact of a less effective vaccine is approximately equal to the impact of a slower rate of administration. According to our model, the two approaches result in an approximately equal number of deaths when Vaccine 2 has an 85% efficacy. Below 85%, more lives are saved with approach A, but approach B leads to more lives saved when Vaccine 2 has an efficacy above 85%.\nConclusion\nWhile the specific numbers predicted by our model will almost certainly vary from the real-life course of COVID-19 in the U.S. in 2021, the most important takeaway from this post is the vital role that vaccine efficacy and rollout strategies will play in determining the pandemic’s course. If and when multiple vaccines become available, policy makers must keep the importance of vaccine efficacy differences in mind as they seek to develop vaccine allocation approaches that maximize lives saved. If a new vaccine has a significantly lower efficacy (< 85% according to our model) compared to the 95% efficacy of Pfizer and Moderna’s, it will be optimal to start administering it to low-risk individuals right away. If the efficacy is similar to 95% (> 85% according to our model), the best strategy is to use it to ensure that the oldest individuals get vaccinated as soon as possible.\nModel Extensions\nThe model framework set up in this post has the potential to become even more reflective of the real-life pandemic with some extra considerations. The true contact matrix and related transmission rate \\(\\beta\\) are changing constantly as a result of developing societal perceptions of the virus, individual behaviors, and government policies. A time-dependent contact matrix and \\(\\beta_t\\) would allow the model to reflect some of these dymanics. Another possible extension of the model could include more demographic classes that take into account other factors besides age such as job or location.\nAs a final point, this model is deterministic in that its projections are based on exact parameters without accounting for randomness. Despite this limitation, deterministic models are still used widely throughout epidemiology and have had some important applications. I am hoping to play around with a stochastic model that incorporates some of these other ideas in the future!\nAlso, let me know if there are any other scenarios I should explore. I am hoping to make a Shiny app that makes any scenario possible!\nAppendix: Notes on Parameters and Initial Conditions\nThe rate of death once infected with COVID in each of the following age groups is (taken as (deaths per 100,000)/(incidence per 100,000) from CDC estimates. Assumes number of patients re-infected is minimal.):\nThe initial conditions for the model were estimated from Statistica population data and New York Times COVID-19 data as of January 3, 2021. They were:\n\n.cl-8f03b954{font-family:'Arial';font-size:11px;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;}.cl-8f03b9e0{font-family:'Arial';font-size:11px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;}.cl-8f03ba08{font-family:'Arial';font-size:7px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(17, 17, 17, 1.00);background-color:transparent;position: relative;bottom:3px;}.cl-8f03ba30{font-family:'Arial';font-size:6px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;position: relative;bottom:3px;}.cl-8f03ba58{font-family:'Arial';font-size:10px;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-8f040206{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-8f040292{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:2px;padding-top:2px;padding-left:5px;padding-right:5px;line-height: 1.00;background-color:transparent;}.cl-8f0402ba{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:0;padding-top:0;padding-left:0;padding-right:0;line-height: 1.00;background-color:transparent;}.cl-8f054f62{width:43px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054f9e{width:28px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054fb2{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054fc6{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054fd0{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054fe4{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f054fee{width:43px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05500c{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055020{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055034{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055052{width:28px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055066{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055084{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055098{width:43px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0550ac{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0550ca{width:28px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0550de{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0550fc{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055110{width:43px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05512e{width:28px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05514c{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05516a{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f055188{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05519c{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 1.00px solid rgba(255, 255, 255, 0.00);border-top: 1.00px solid rgba(255, 255, 255, 0.00);border-left: 1.00px solid rgba(255, 255, 255, 0.00);border-right: 1.00px solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0551ba{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0551d8{width:50px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f0551ec{width:28px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05520a{width:43px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05521e{width:34px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-8f05523c{width:131px;background-color:transparent;vertical-align: middle;border-bottom: 2.00px solid rgba(0, 0, 0, 1.00);border-top: 2.00px solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}AgePopulation (in Millions)SV1V2IAISRD0-4 19.58 19.09 0 0 0.05 0.05 0.39 0 5-9 20.19 19.69 0 0 0.05 0.05 0.40 0 10-14 20.80 20.28 0 0 0.05 0.05 0.42 0 15-19 21.06 20.53 0 0 0.05 0.05 0.42 0 20-25 21.63 21.09 0 0 0.05 0.05 0.43 0 26-30 23.50 22.91 0 0 0.06 0.06 0.47 0 31-35 22.43 21.87 0 0 0.06 0.06 0.45 0 36-40 21.73 21.19 0 0 0.05 0.05 0.43 0 41-45 19.92 19.42 0 0 0.05 0.05 0.40 0 46-50 20.40 19.89 0 0 0.05 0.05 0.41 0 51-55 20.48 19.97 0 0 0.05 0.05 0.41 0 56-60 21.87 21.32 0 0 0.05 0.05 0.44 0 61-65 20.57 20.06 0 0 0.05 0.05 0.41 0 66-70 17.46 17.02 0 0 0.04 0.04 0.35 0 71-75 14.03 13.68 0 0 0.04 0.04 0.28 0 76-80 9.65 9.41 0 0 0.02 0.02 0.19 0 81-85 6.32 6.16 0 0 0.02 0.02 0.13 0 85+ 6.61 6.44 0 0 0.02 0.02 0.13 0  All numbers are in millions\n\nThe models also assumes that:\nThe death and infection rates published in the CDC ACIP Report on 12/20/20 are accurate and uniform within their defined age groups.\nThe vaccine is effective at preventing illness and transmission.\nThe contacts made by individuals are reduced by a constant multiplier as a result of social distancing guidelines. The nature and proportional allocation among age classes does not change. This is probably not true, but I am not aware of studies that have quantified the adjusted contact rates.\nNone of the parameters change over time. This is not true, as I assume/hope more extreme measures would be taken to slow the growth if it exploded.\nStochastic effects do not affect the course of the pandemic.\n\nReferences\nMorris SE (2020). shinySIR: Interactive Plotting for Mathematical Models of Infectious Disease Spread. R package version 0.1.2, https://github.com/SineadMorris/shinySIR.\nPrem K, Cook AR, Jit M (2017) Projecting social contact matrices in 152 countries using contact surveys and demographic data.\nTowers, S. (2012, December 11). SIR Model with Age Classes. Retrieved January 02, 2021, from http://sherrytowers.com/2012/12/11/sir-model-with-age-classes/\nTowers, S., & Feng, Z. (2012). Social contact patterns and control strategies for influenza in the elderly. Mathematical biosciences, 240(2), 241-249.\nComputing Note: All analysis was performed in R statistical software. The package deSolve was used to solve each system of differential equations. Plots were created with ggplot2. All code used in this analysis is available here.\n\n\n\n",
    "preview": "posts/2020-12-28-what-if-some-vaccines-are-more-effective-than-others/what-if-some-vaccines-are-more-effective-than-others_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-01-24T18:02:09-06:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 2304
  },
  {
    "path": "posts/2020-12-15-a-guide-for-interpreting-covid-data/",
    "title": "A Guide for Interpreting COVID Data",
    "description": {},
    "author": [],
    "date": "2020-12-15",
    "categories": [],
    "contents": "\n\n\nNote: I originally published this article on a previous blog on March 22, 2020. We’ve leanred alot since this, but I have posted this here to show where my thoughts were way back then!\n\n\nCOVID-19 has put most of the world on lockdown, and new data is constantly being released and interpreted by internet “experts” telling you what insights it reveals into our future. While some of it is true, the data is raw, biased, and generally problematic. With the right background knowledge, it can still, however, be useful. This is a guide to avoiding the biggest mistakes when it comes to interpreting Coronavirus data.\nCases vs. Confirmed Cases: Who has been tested\nMy twitter feed is crawling with daily updates about case counts in different counties, states, and countries. I can always count on CNN’s daily update of their “state-by-state breakdown of US Coronavirus cases” to be somewhere among them. Tens of thousands of users can click on this article and find an updated chart titled “Coronavirus Cases in US”, a portion of which can be seen below:\n\nSource: CNN. Data as of March 22, 2020.\nWhile it is informative, this chart is quite misleading. By placing these case numbers next to each other in this way, the chart is implying that the states can be directly compared. It even sorts them in descending order, emphasizing the fact that New Jersey is reporting slightly more cases than Washington and almost twice as many as Michigan.\nThe most important piece of information on this chart is in the fine print under the title where it states that the data are confirmed cases. Well, obviously, you might think, but what does that really mean? The only way to confirm that someone has COVID-19 is for them to test positive for it. In the United States especially, these tests are hard to come by. Even more, each state has its own protocols for deciding who is able to get tested, and different testing sites employ different labs to perform the analyses which leads to widespread inconsistencies in reporting. Below is a chart from Politico that includes this other r extremely important piece of information.\n\nSource: Politico. Data as of March 22, 2020.\nHere we see that testing varies significantly across states. Whereas the CNN article suggested that Washington State was dealing with approximately the same number of Coronavirus cases as New Jersey, the addition of testing as a detail shows that their true case numbers might actually be quite different.\nAt This Point We Can’t Really Compare Across States\nWhile it seems easy to draw from this information that New Jersey’s true case prevalence is much greater than Washington’s, we really can’t be sure. New Jersey is being much more conservative with their testing than Washington State at the moment since they hit 100 cases much more recently than Washington, and they are only beginning to ramp up testing now.\nCurrently, 83.6% of the 2,290 tests in New Jersey have come back positive. If they were to conduct 25,000 more tests today, that percentage would undoubtedly become much lower, but there is no way of telling exactly how much lower without actually testing.\nIf we want to be able to make accurate inferences about entire populations, we either need to sample everyone or use a random sampling technique. The current rate of testing is far from either of those, especially considering the fact that many COVID-infected individuals are known to be asymptomatic and aren’t even allowed to get tested. Still yet, we might at least be able to compare confirmed cases across states if testing rates were similar, but we can see that they aren’t even remotely close. The bottom line is don’t let any media outlets, Facebook friends, or the President of the United States fool you by saying there is any certainty here. Any comparison of Coronavirus numbers should be taken with a large grain of salt.\nComparisons Across Countries Are Slightly Better\nYou have probably heard the Coronavirus trajectory of the U.S. being compared to that of other countries hit hard by the virus including China, Italy, and South Korea. Below is a graph showing the path of each country’s confirmed cases count from the day they hit 500 confirmed cases.\n\nData from JHU CSSE and available here.\nHere we see that the United States is following a dangerous path. Even though testing procedures and rates vary noticeably across these four countries, the sample sizes are much larger than the state-by-state data from the previous section, and the trends this data captures are fairly reflective of the true infection curves.\nSouth Korea actually has the highest testing rate of any of these countries despite reporting the least number of confirmed cases by far. This is evidence that their low confirmed case tally is not due to a lack of testing, but instead thanks to a quick and effective public health response in comparison to China, the United States, and Italy.\nComparisons across countries similar to the one illustrated in the graph above are useful in assessing the relative effectiveness of public health responses. Still, they are not perfect and must also be taken with a grain of salt.\nUsing numbers to assess your own danger\nAlmost everyone knows someone who is in a vulnerable population group, and the thought of becoming infected with COVID-19 as a healthy young person is still very frightening. It is reasonable to be concerned by the exponential rise of recorded case numbers, but it is important to remember that they are summarizing an extremely complex pattern. Just as a case mortality rate of 1.3% does not mean that every person has an equal chance of dying from COVID-19, a nationwide doubling of cases does not necessarily mean the same for everyone in the country. Since most of the case increase is coming out of New York right now, an immunocompromised 80-year-old in Arizona who has been strictly following social distancing protocols has much less to be worried about than a similar New Yorker who only stopped using the subway a few days ago.\nPopulation-level data is valuable, but it fails to capture key details of the subdivided social, geographical, and economic landscape of this massive country. We need to closely monitor COVID-19 as testing becomes more widespread to see how it disproportionately affects certain groups such as the homeless, undocumented immigrants, and public service workers. The most important patterns in the data are not discernible in the overall picture.\nWhat to do the next time you come across Coronavirus data\nRemember that every piece of Coronavirus data is imperfect and comes with strings attached. Anyone who tries to tell you differently is misleading you. The next time you come across a bit of reported data, question it. Ask yourself what information is missing, what unmeasurable factors could be controlling it, and if the message its presenters are sending is necessarily true. The best tool for navigating through the era of misinformation is skepticism.\nFor more information, check out the COVID Tracking Project\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-15T23:18:29-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-15-an-introduction-to-wins-above-replacement/",
    "title": "What's Wrong with WAR?",
    "description": "A short description of the post.",
    "author": [],
    "date": "2020-12-15",
    "categories": [],
    "contents": "\nAsk just about anyone and they will tell you that the most important thing is sports is winning. Of course, individual awards are always on certain players’ radars (whether they admit it or not), but fans, managers, and owners are primarily invested in maximizing team W’s. The recent explosion of sports analytics has given rise to a countless number of formulas, ratings, and “advanced metrics” that claim to perfectly measure individual performance, but how do these stats translate to actual team performance?\nWAR in Baseball\nBaseball’s WAR (Wins Above Replacement) has risen to become one of the most widely used advanced metrics across all of sports. Different websites have slightly different ways to calculate WAR, but each essentially defines it as a measure of how much value a player provides to his team relative to the average “replacement” player. For the purpose of this article, I will focus specifically on Baseball Reference’s version because I feel most loyal to their site. I won’t go into too much detail about exactly what it is and how its calculated, so if you want to read more, you can here.\nHundreds of factors go into measuring WAR from batting stats to fielding stats to baserunning stats to positional adjustments to seasonal run environment adjustments. Sabermetricians have spent the past decade tweaking the stat to make it more and more precise, and it’s popularity has steadily risen as it has made its way onto ESPN broadcasts, official MLB tweets, and ballpark scoreboards. Today, WAR is a central part of even the casual fan’s vocabulary and some regard it as baseball’s perfect catch-all stat. But is it? The short answer is no. Let’s take a look at some of the reasons why.\nLuck\nLuck plays a massive role in every part of our lives and sports are no exception. It doesn’t take an expert to see how Franco Harris’ immaculate reception changed the course of the 1972 NFL Playoffs or more recently how the four perfect bounces on Kawhi Leonard’s game 7 lead to Joel Embiid in tears and two new Drake tracks. In baseball, luck plays an especially enormous role in every play. Rafael Devers may rip a 110 mph liner that happens to land in Justin Smoak’s glove before he even has time to blink, while in Yankee Stadium, Aaron Judge can hit a weak pop-up off the end of his bat that would be an easy out almost anywhere else and it goes down on paper as a homerun (the most valuable play in baseball). Just as batting average is susceptible to giving players credit they don’t deserve (or didn’t earn depending on how you look at it) and vice versa, WAR, which factors in a weighted version of on-base percentage, doesn’t capture which players are getting “lucky”. With this being said, the scores/results/records also give pop-ups like Aaron Judge credit and punishes lineouts like Devers’. While it might be frustrating to fans and players, luck is a fundamental part of baseball and no stat will ever be immune to it.\n\n\nNote: Batting Average on Balls in Play (BABIP) is typically used to measure how “lucky” a player has been getting (and to identify whose batting average might be about to change once their sample size increases), but it still has imperfections. For example, 2018 NL MVP Christian Yelich has finished with a BABIP in the top 11 among qualifiers in 4 of his 5 complete seasons so far. This is less an indicator of him being very lucky than a sign that when he makes contact, it’s pretty hard hit.\n\n\nClutch?\nOne of the biggest criticisms of comprehensive stats like WAR is that they fail to account for the situation. There is no doubt that a three-run home run to put your team up by one in the top of the 9th is more important than a solo shot in the late innings of a blowout, but WAR treats them each the same. Does making a play in a bigger moment make someone more valuable? We are Among sports analysts, the concept of “clutch” is a frequent source of debate. Regardless of whether it exists or not, incorporating situational measures (that are out of an individual player’s control) into WAR calculations gives players on better teams or happen to have had a lot of at-bats in big situations an unfair advantage. Non-contextual measures give one standardized statistic for players across the league to be compared on. Still, if you believe in the clutch factor, the existence of WAR doesn’t mean you have to ignore a player’s .220 batting average with RISP when voting for MVP.\nMoney\nWAR represents the value of a player relative to the average replacement player. But if a certain player wasn’t around that doesn’t necessarily mean Joe Smith from triple A would be playing his position instead. It might if the certain player was temporarily injured (and that is what WAR should be interpreted as meaning), but baseball reference has more recently included WAA (wins above average) in many of their tables to show how a player compares to the league average. Baseball Reference explains that the issue with WAA is that a league average player is relatively expensive and is not typically bought to replace an injured starter. As seen by this definition, WAR is focusing on a player’s value healthy vs. injured. But for the free agent market, trade negotiations, and MVP voting, is this the best comparison to be making? For owners (generally speaking), baseball is a business, and that means they want to maximize performance at the lowest necessary cost. Below is a table displaying the most valuable position players of 2018 relative to their salary.\nName Team WAR Salary Salary/WAR Matt Chapman OAK 8.2 $547,500 $66,728.29 Francisco Lindor CLE 7.9 $623,200 $78,886.08 Alex Bregman HOU 6.9 $599,000 $86,811.59 Trevor Story COL 5.6 $555,000 $99,107.14 Whit Merrifield KCR 5.5 $569,500 $103,545.45 Unsurprisingly, the top 5 of this list is made up of young stars that have yet to hit arbitration and are still at or near the league minimum. If we want to see how Major League front offices value WAR, the free agency market may be a better place to look. Below is a table of the most valuable position players of 2018 relative to their salary that were acquired through free agency.\nName Team WAR Salary Salary/WAR David Peralta ARI 3.9 $3,300,000 $846,154 Lorenzo Cain MIL 6.9 $14,000,000 $2,028,985 Mike Moustakas KCR/MIL 2.5 $5,500,000 $2,200,000 J.D. Martinez BOS 6.4 $23,750,000 $3,710,938 Nick Markakis ATL 2.6 $11,000,000 $4,230,769 To get a better sense of how the league as a whole values WAR, here is a graph displaying the relationship between 2018 WAR and average yearly salary for contracts signed in 2019 .\nr2 = 61.4%\nThe association between WAR and salary is fairly strong. In the 2019 market, one WAR was worth about $3.45 million on average. But what exactly does one win above replacement mean?\nSo How Does it Relate to Wins?\nAs far as owners and fans are concerned, the only thing that matters is team performance, and team performance is measured with one very simple stat: wins. The first letter of the WAR acronym stands for “wins” so it is expected that the two are closely related. Actually, WAR is more directly measuring value added in terms of runs, not wins, but the correlation between runs and wins allows for a simple transformation for a final measurement that is true to its name. The basis of this transformation comes from baseball’s pythagorean formula: expected wins = runs scored2/(runs scored2 + runs against2). It is roughly comparable to goal difference in soccer and has similar flaws. Still, a teams actual win-loss record is typically pretty close to what would be expected based on the formula. Below are graphs showing the association of a team’s total WAR with pythagorean wins and actual wins.\nr2 = 87.24%, 94.82%\nAs you can see, WAR is pretty strongly associated with Pythagorean wins and only slightly less correlated with actual wins. Still, the association is not perfect (and you shouldn’t expect it to be). The outcome of a baseball game is determined by a multitude of factors regarding player performance and beyond that are not captured by WAR.\nBill James Controversy\nIronically, the father of sabermetrics himself has been one of WAR’s biggest critics. In a 2017 article published on his personal website, Bill James gave his take on how WAR was being misused in the 2017 AL MVP argument, sending the sabermetric community into a brief period of unrest. Essentially, James’ two biggest ailments with the measure have to do with its disregard for the situation and resulting disconnect from wins. I covered the gist of the issues here, but there is plenty of further reading a quick google search away.\nConclusion\nThere is no such thing as a perfect stat. WAR is an incredible metric that captures factors of player value that no stat before has ever been able to capture and places it into one number that can be used to compare players across leagues and positions. With that being said, it also leaves out a number of factors, some of which I outlined above and others that may not even realize. When it comes to winning ball games, the team whose players have the higher combined WAR may win more often than not, but this is not always the case. The ins and outs of baseball have been quantified, analyzed, and beaten senseless by experts for decades, but there is still so much that we can’t understand let alone measure. That’s what makes it beautiful.\nTo be clear, I am a big proponent of WAR and use it heavily myself when assessing an MLB player’s value. Still, I think Bill James and others have made reasonable criticisms of it and advise readers to be cautious the next time it comes up as a be all end all stat in an MVP debate.\n\n\nNote: Baseball Reference released their 2020 WAR update after the completion of this article, so WAR values may now be slightly off.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-15T21:14:06-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-15-bayes-theorem-and-covid19/",
    "title": "COVID-19 Testing and Bayes' Theorem",
    "description": {},
    "author": [],
    "date": "2020-12-15",
    "categories": [],
    "contents": "\nIn the nine months since COVID-19 diagnostic tests were first made available to the public in March, they have served valiantly as one of the world’s greatest tools for tracking the sweeping spread of the disease. Widespread testing has also enabled the return of professional sports, made the partial re-opening of many college campuses possible, and provided a sense of safety for small social gatherings.\nBut just how reliable are they? The answer to this question carries much more weight at the level of the individual in that a few inaccurate results out of 100 won’t change much in terms of health officials’ overall tracking of the disease, but it could be the difference between life and death of a loved one for the few that the error affects. Studies have suggested the true error rate is probably somewhere around this “few in a hundred” mark, but the accuracy can vary depending on other factors such as the type of test and time since exposure. For this reason, CDC guidelines clearly state that a negative test doesn’t clear someone from the possibility of being infected with COVID, but many people are still willing to act like it does. In some cases, this may be a reasonable risk to take, but a more thorough assessment of the risks requires an understanding of the importance of prior probability.\nBayes’ Theorem and COVID Testing\nReally, a COVID test should be treated as no more than a (quite strong) piece of evidence in a larger pool of information. If person A, who had a lengthy indoor dinner with a symptomatic friend, decides to get tested after experiencing symptoms and gets a positive result, they can be almost certain that they have COVID. If person B, who follows social distancing guidelines stricly, gets a precautionary test that comes back positive, there is still a fair chance that they have COVID, but their probability of being infected is much lower than person A’s. The only difference between these cases is the “other” evidence available in the larger pool, and it matters. Bayes’ Theorem helps us understand why and allows us to quantify the difference. In its simplest form, the theorem can be written as: \\[P(A|B)= \\frac{P(A)*P(B|A)}{P(B)}\\]\nWhere:\n\\(P(A|B)\\) is the updated, or posterior, probability of \\(A\\) given the evidence \\(B\\).\n\\(P(A)\\) is the prior probability of \\(A\\) before considering the evidence \\(B\\).\n\\(P(B|A)\\) is the conditional probability of the evidence \\(B\\) occurring supposing that the outcome \\(A\\) is in fact true.\n\\(P(B)\\) is the prior probability of the evidence \\(B\\) occurring before it actually occurred. This is the “universe” of our equation, which we are dividing to take only the portion where \\(A\\) is true.\nEssentially, Bayes’ theorem tells us how much a new piece of evidence alters the probability we assigned to the outcome before receiving the new evidence. This altered probability is called the posterior probablility, or \\(P(A|B)\\).\nApplying this theorem to our COVID testing case, we have \\[P(COVID^+|test\\:result)= \\frac{P(COVID^+)*P(test\\:result|COVID^+)}{P(test\\: result)}\\]\nWhile it may not look it, Bayes’ Theorem it is actually quite intuitive in this simple case. This becomes especially apparent when we consider the denominator, \\(P(test\\:result)\\), as the sum of two distinct parts. Because we don’t know for certain whether we have COVID, the test result could be false or true, so one part must account for the probability that the the result occurs and we have COVID and the other part for the probability that the result occurs and we don’t have COVID. Accordingly, we can break \\(P(test\\:result)\\) into the sum of \\(P(COVID^+)*P(test\\:result|COVID^+)\\) and \\(P(COVID^-)*P(test\\:result|COVID^-)\\), giving us a new application of Bayes’ Theorem to COVID:\n\\[\\scriptsize P(COVID^+|test\\:result)= \\frac{P(COVID^+)*P(test\\:result|COVID^+)}{P(COVID^+)*p(test\\:result|COVID^+)+P(COVID^-)*P(test\\:result|COVID^-)}\\]\nNow it is clear to see that this theorem simply represents the proportion of total scenarios where we receive a specific test result in which we also have COVID.\nIntepreting COVID Test Results: An Example\nScenario One: Let’s suppose that you work from home, strictly follow social distanning guidelines, and avoid leaving the house at all costs but live with one roommate who recently traveled out-of-state to visit family. Upon their return, they start to develop a fever and decide to get a COVID test which comes back positive. You isolate from them immediately and decide to get a Rapid Antigen test five days (to increase the chances of an accurate test by allowing the virus to incubate) later. It comes back negative. What is the probability that you have COVID?\nRecalling our previously outlined Bayesian approach to this question, we need to be able to estimate three probabilities:\n\\(P(COVID^+)\\): The prior probability of you having COVID prior to the test. Since you never leave the house, it is nearly certain that the only way you could have been infected with the virus was from your roommate. A recent study estimated the secondary transmission rate of COVID-19 within households to be 53%. This estimate is certainly not perfect, but it does’t have to be, so you reason that your prior probability of having COVID before receiving the test result is 0.53. Notice that because you either have COVID or you don’t, \\(P(COVID^-)\\) is 1-0.53 = 0.47.\n\\(P(test\\:result|COVID^+)\\): The probability that you would test negative given that you have COVID. This probability is called the “false negative rate” of the test. Digging into the research, you find a study that estimates a false negative rate of 20% for the specific test that you received (not bad for a rapid test given that the false negative rates of rapid antigen tests are thought to be anywhere from 10%-50% according to a Harvard Medical School Blog Article). Again, this estimate is probably not perfect, but it’s the best that you have.\n\\(P(test\\:result|COVID^-)\\): The probability that you would test negative given that you do not have COVID. This probability is called the “specificity” of the test. The same study that you used to find the false negative rate estimates the true negative rate to be 95%.\nPlugging these numbers into Bayes’ Theorem, you have:\n\\[\\small \\begin{split} P(COVID^+|test^-)  & = \\frac{P(COVID^+)*P(test^-|COVID^+)}{P(test^-)} \\\\ & = \n\\frac{P(COVID^+)*P(test^-|COVID^+)}{p(test^-|COVID^+)*P(COVID^+) + p(test^-|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.53)(0.20)}{(0.20)(0.53) + (0.95)(1-0.53)} \\\\ & =\n0.192\n\\end{split} \\]\nWith this information, you determine that based on the evidence you have, there is still a 19.2% chance that you have COVID.\nScenario 2: Now suppose that your living situation is the same as scenario one, only you do not have a roommate. You can’t imagine that you have the virus given that you never leave the house, but you want to visit your parents for Thanksgiving, so you decide to get a PCR COVID test before leaving just to be safe. To your surprise, it comes back positive. What is the probability that you have COVID?\nFirst, we estimate the probabilities for Bayes’ Theorem:\n\\(P(COVID^+)\\): The prior probability of you having COVID prior to the test. Before getting the test, you thought that it was almost impossible for you to have the virus. If you do have it, you reason that you must have picked it up through a surface transmission on a grocery delivery. This type of transmission is known to be rare, and you usually wash your hands after touching any deliverires. You research to find an estimate that 3% of your county is currently infected with COVID and use that to assign your delivery driver a 3% chance of carrying COVID. You then find research that suggests the probability of you getting COVID from touching the same surface as an infected individual is 5%. Thus, you assign a prior probability of (0.05)(0.02) = 0.001, or 0.1%.\n\\(P(test\\:result|COVID^+)\\): In this scenario, this value is the probability that you would test positive given that you have COVID. This probability is called the “sensitivity” of the test. Digging into the research again, you find a study that estimates a false negative rate of 2% for the PCR COVID tests. If the false negative rate is 2%, that means that the true positive rate is 98%. If 2% of people who have COVID return a negative (falsely), the other 98% must positive (accurately).\n\\(P(test\\:result|COVID^-)\\): The probability that you would test positive given that you do not have COVID. This probability is called the “false positive rate” of the test. The same study that you used to find the test sensitivity estimates the true negative rate to be 95%. You find that an estimated false positive rate of 0.5% for PCR tests.\nPlugging these numbers into Bayes’ Theorm Formula, you have:\n\\[\\small \\begin{split} P(COVID^+|test^+)  & = \\frac{P(COVID^+)*P(test^+|COVID^+)}{P(test^+)} \\\\ & = \n\\frac{P(COVID^+)*P(test^+|COVID^+)}{p(test^+|COVID^+)*P(COVID^+) + p(test^+|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.001)(0.98)}{(0.98)(0.001) + (0.005)(0.999)} \\\\ & =\n0.164\n\\end{split} \\]\nWith this information, you determine that based on the evidence you have, there is only a 16.4% chance that you have COVID.\nAnother Interesting Bayesian Message\nIn scenario 2 above, you conclude that the test result is more likely a false positive than a true positive despite the 0.5% false positive rate. Applying this to a larger scale, let’s suppose that a hospital implements a large-scale testing program where each of their 2,000 employees where each is tested weekly for COVID. They strike a deal with a PCR testing company that produces a test with a reported false-positive rate of 0.1% and a false-negative rate of 3% (note that we don’t have great estimates for the true rates for these tests). The COVID case rate in the area is low, and the previous COVID-monitoring program suggested that very few workers had been infected so far over the course of the pandemic. In other words, their COVID-prevention protocols seem to be working.\nAssuming that each worker takes similar precautions, we can assign each of the 2,000 the same underlying (prior) probability of being infected with COVID each week. Since their protocols have been effective, and other evidence suggests the employees are much more careful than the average person, we approximate that this probability is 0.12%. We now have all the pieces of Bayes’ Theorem, which we can use to estimate that the posterior probability of COVID infection for an employee that tests positive is only 0.371.\n\\[\\small \\begin{split} P(COVID^+|test^+)  & = \\frac{P(COVID^+)*P(test^+|COVID^+)}{P(test^+)} \\\\ & = \n\\frac{P(COVID^+)*P(test^+|COVID^+)}{p(test^+|COVID^+)*P(COVID^+) + p(test^+|COVID^-)*P(COVID^-)} \\\\ & =\n\\frac{(0.0012)(0.97)}{(0.97)(0.002) + (0.0012)(0.998)} \\\\ & =\n0.371\n\\end{split} \\]\nThis result means that almost 2/3 of employees that return positive tests each week do not actually have COVID. Since a positive test, regardless of the prior probability, likely means two weeks away from work for the potentially infected individual and their close contacts, this can have some harmful implications for the hospital’s operations. On the other hand, the increased chance of catching the two true positives may be worth the consequences.\nRegardless, this example illustrates the risk of over-testing a group of individuals that have a very low prior probability of being infected. If the underlying case rate is low, most positives are actually false, even if the false positive rate is small.\nClosing Remarks\nIn reality, every individual either does or doesn’t have COVID. The probability that we are calculating is simply our best guess for the likelihood that someone has COVID based on the evidence that has been collected on the matter*. This probability can prove to be quite valuable when it is used to informing decision making around the disease. Remember, though, that the cost of spreading COVID is extremely high, so it best practice to stick to the side of caution. If there is even a small chance that you have COVID, quarantine and follow CDC guidelines!\n\n\n*If we wanted to be even more robust in our application of Bayes’ Theorem to COVID, we would use probability densities to capture the uncertainty at play in our determination of priors. Maybe in a future post.\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-23T22:10:54-06:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome to my site",
    "description": {},
    "author": [],
    "date": "2020-12-15",
    "categories": [],
    "contents": "\nWelcome\n\n\n\n",
    "preview": {},
    "last_modified": "2020-12-15T17:34:48-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-15-whats-the-deal-with-covid-racial-data/",
    "title": "What's the Deal with COVID Racial Data?",
    "description": {},
    "author": [],
    "date": "2020-04-22",
    "categories": [],
    "contents": "\n\n\nNote: I originally published this post on a previous blog on April 22, 2020.\n\n\nIt is no secret that the United States has an ugly history of racial disparities in health outcomes. While some might argue that our nation has made significant progress in this realm, evidence has shown that the factors underlying these inequities are so deeply ingrained in our society that we have barely reached their surface. So as COVID-19 makes its way to the top of America’s leading causes of death list, it is no surprise that communities of color are being impacted at a higher rate. Still, data detailing the racial differences in testing, cases, and deaths is scarce. But why? And what do we know so far?\nWhere is the data?\nThe CDC released its first COVID-19 report including breakdown of cases by race on April 17th, and it only contained racial data for 25% of cases. As of April 21st, they have racial data available for less than 40% of reported deaths from COVID-19. The primary reason for this gap in data is the federal system’s reliance on cause of death codes from registered death certificates. Remarkably, a code specifically for COVID-19 was not added until March 24th, when the total recorded number of cases in the U.S. was already above 54,000. This, combined with the lags in reporting and processing of data, makes the federal numbers quite unreliable when it comes to demographics.\nWhile federal data lags far behind the action on the ground, many states and counties are publishing their own data in a much timelier fashion. According to John Hopkins University, 35 states are now reporting racial data on confirmed COVID-19 cases. Of those, 29 states are also reporting it for deaths, but only Illinois and Kansas have released racial data for tests. These numbers aren’t great, but they are much better than anything we had a few weeks ago. APM research lab estimates that we now have data about race for 60% of Coronavirus deaths in the United States. Still, it is currently only available on a state-by-state basis. The COVID Tracking Project is working on a racial data tracker that should make the data easier to work with, but they expect it to be a couple of weeks before it is ready.\nWhat do we know so far?\nWe know with certainty that blacks in particular are being disproportionately affected by COVID-19 in the United States. The evidence for this is already so convincing that Donald Trump even admitted something needed to be done about it. At a national level, the CDC reports that blacks, who made up 12.8% of the U.S. population in 2018, accounted for 19.9% of the official COVID deaths as of April 21st. While this is a notable difference, the last section discussed why this federal data might not be too reliable. Let’s now take a look at some more reliable numbers from the states that have been hit the hardest:\n\nBased on COVID data released by state public health departments. Population data retrieved from the US Census Bureau.\nThis chart illustrates a striking pattern in the state-by-state data so far: blacks are being disproportionately killed by COVID-19, and it’s not even close. The states typically report these numbers with an asterisk indicating that the data is incomplete. While this is true, most are reporting racial data on well over 75% deaths. The patterns are clear. Looking even closer at some of the cities that have been hardest by COVID-19, the same frightening trends hold.\n\nBased on COVID data officially released by cities. Population data retrieved from the US Census Bureau.\nGiven the long history of inequitable health outcomes in the United States, these findings should hardly come as a surprise. But that doesn’t make them any less horrifying. This is convincing evidence that something needs to change right now in the way we deal with the COVID crisis and the ways in which we approach public health in America.\nThe Causes\nThis section could be a book, and there are definitely hundreds of books out there on this specific topic. I won’t go into any detail because I can’t even fully grasp, let alone try to summarize, how ingrained these causes are in our society, policies, and institutions. I encourage further reading.\nI will, however, make one point on this topic. The CDC explicitly states on their COVID demographic data home page that the deaths from the virus are concentrated in specific regions of the U.S. that do not reflect the racial distribution of the nation as a whole, and the distribution of deaths should therefore not be compared to the racial distribution of the U.S. population when determining if certain subgroups are being affected disproportionately. The first part of that statement is certainly true, but the second part is boderlining absurdity. We have seen crystal clear evidence that the virus spreads most viciously in places where people live in close quarters, can’t afford to social distance, and have environmental hazards that cause pre-existing conditions. We have also seen crystal clear evidence that these places (which include but are not limited to inner cities and prisons) are disproportionately non-white. It is absurd for the CDC to ask us to ignore these facts. The disproportionate distribution of COVID cases needs to be compared to racial distribution of the U.S. population because it highlights the deep-rooted racism in our health system. The fact that people of color are concentrated in the places most vulnerable to infectious disease is not a coincidence.\nOther Disparities\nWhile pressure on states to collect and publish data on the racial burden of COVID-19 has been steadily mounting, other important sources of inequity have received noticeably less attention. Health disparities exist across many dimensions including sexual orientation, gender identity, disability status, and socioeconomic status. It’s hard to know if we will ever get data on these factors, but they would undoubtedly reveal deeper insights into the social determinants of COVID outcomes.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-23T22:10:32-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-12-15-why-will-it-take-so-long-to-get-a-covid-vaccine/",
    "title": "Why Will it Take so Long to Get a COVID Vaccine?",
    "description": {},
    "author": [],
    "date": "2020-03-24",
    "categories": [],
    "contents": "\n\n\nNote: I originally published this post on a previous blog on March 24, 2020. It’s pretty cool that today is the second day of vaccine administration in the US!\n\n\nAs I write this post, the global COVID-19 recorded cases count has surpassed 350,000, and that number could easily become 1 million within the next couple weeks. For all we know, it could already be much closer to that point given the extreme amount of unreported cases. The United States and most of Europe have imposed extreme social distancing measures to slow the spread, but it seems that we have long passed the point where the virus dying out on its own is a possibility anytime soon.\nBarring an extreme twist, humans will need to develop herd immunity to the virus in order to stop its spread. That is, approximately two-thirds of us must become immune to COVID-19 in order to stop its transmission among the population. The only two ways for that to happen are for two-thirds of us to contract it, a devastating scenario, or for a vaccine to be developed and distributed extensively. Scientists across the world have been scrambling to make the latter scenario reality ever since Chinese researchers discovered and released the virus genetic sequence in January and governments have rightfully poured funding behind them. Still, NIAID director Anthony Fauci has made it clear that a best-case scenario is that the vaccine will be ready in 12 to 18 months. With so many resources being poured into this project, why can’t it be any quicker?\nDeveloping a Vaccine\nOver 30 labs across the world have already developed vaccine products that might be effective in generating immunity to the novel Coronavirus. In general, they employ techniques used in established vaccines such as the one for influenza which involve exposing the immune system to weakened versions or pieces of the virus to generate antibodies that will allow the body to recognize and fight off the actual virus if it becomes infected. The majority of these vaccines are in the pre-clinical stage meaning that they have not yet been tested in humans. In other words, we don’t know if they are safe, let alone if they work. We have a long way to go until we reach those points.\nClinical Trials\nClinical trials are studies conducted in biomedical research to assess the safety and efficacy of a health intervention. Through a number of lengthy phases, human subjects that receive the treatment are closely followed and compared to control subjects that receive a placebo or the industry standard treatment. Studies have shown that the average time for a product to reach regulatory approval from the moment it is first tested in humans is around seven years, and the associated costs are approximately $1 billion. In general, there are three main phases of clinical trials that each new product must go through before becoming eligible for government approval, which is required before it is made available to the general public:\nPhase One: The treatment is first introduced in humans in phase one. It has typically already gone through animal testing. 20-80 healthy subjects are recruited for this phase and they are closely monitored for side effects. The goal in phase one is simply to assess safety, so the actual efficacy of the drug is not usually measured. Different doses are also frequently tested here to help identify how much of a drug can safely be given to subjects in future phases.\nPhase Two: After the treatment has passed through phase one, it is moved into phase two where a few hundred actual patients are given the treatment. Different dosages are compared to a control, and the researchers still concentrate on safety while beginning to look at efficacy, specifically in terms of best dosage and determining the variables that it seems to be best effecting.\nPhase Three: If the first two phases suggest that the treatment is safe and shows signs of efficacy, it is passed to phase three. A phase three clinical trial typically enrolls 1,000-3,000 patients that are randomized to the treatment or a control. This is the final stage before approval, so it must demonstrate that the treatment is effective for it to be released to the public. Safety is still measured at this stage, especially rare adverse events that the larger sample size increases the chances of detecting.\nThese phases are part of the standard protocol that every vaccine must go through. Each one is important and rushing through them means risking the release of a dangerous drug into the entire population. If a vaccine causes potentially fatal side-effects three months after being administered 1 in 150 times, the trial period is meant to catch that before it kills thousands of people. Some drugs, like the yearly flu vaccine, can be rushed through this process because each new annual version is only a tweaked version of previous ones. The vast majority of other drugs never make it through at all.\nThe severity of COVID-19 is not enough to warrant putting people at even more risk by completely rushing it through this process. It is important to make sure that the vaccine is safe and effective before nations begin pouring billions of dollars into getting it mass produced and administered. Some parts of the process such as bureaucratic tasks and subject recruitment can be expedited, but there is no getting around the time required to see if the vaccines actually work safely. Getting through the trial stage alone in a year and a half will be extremely difficult, and that is only half the battle.\nMass Production and Distribution\nEven if a vaccine successfully makes it through clinical trials in the next year and a half, it needs to be mass produced and widely distributed for it to have any meaningful benefit to our population. This will be no simple task. The lab that produces the vaccine will likely not have the capabilities to mass produce it, so a larger manufacturing company will need to step in to take over the multi-billion dollar process. Given the situation, governments will probably play a heavy role in the funding and execution of this stage, but they might be concerned by the distinct possibility that the virus finishes its natural course between the time that the money is spent and time that the vaccine is mass administered. Regardless, such a large-scale production will take months to complete.\nAt this point the question becomes who gets access to the vaccine first. Is it healthcare workers who are frequently exposed and are the most important players in fighting the disease? Is it the elderly who are most likely to become fatally ill? On top of that, will everyone get it for free? If we reach this point, these are all questions that will be asked and heavily debated across the world. We can hope that this won’t slow down potentially life-saving administrations, but the United States’ record of healthcare politics shouldn’t make us too certain.\nWhat Does this Mean for Us?\nThe best thing we can do to keep ourselves and our loved ones safe while we wait for a vaccine to come out is take proper sanitary precautions and social distance. If that isn’t enough for you, see if you can volunteer to be a subject in a clinical trial. Experts across the world are working extremely hard to find a safe and effective vaccine, but time will be required before it can be released no matter how many resources we pour behind it.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-23T22:09:19-06:00",
    "input_file": {}
  }
]
